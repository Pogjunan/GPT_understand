{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer (Attention Is All You Need) Overview**\n",
    "\n",
    "This notebook provides a concise summary of the seminal paper \"[Attention is All You Need (2017)](https://arxiv.org/abs/1706.03762)\" by Vaswani et al., which introduced the Transformer architecture. The goal is to explain the core concepts and mathematical formulations (in English) without any images.\n",
    "\n",
    "## **1. Introduction**\n",
    "Traditional sequence-to-sequence models relied on recurrent or convolutional structures, which made it difficult to process sequences in parallel and capture long-range dependencies efficiently. The Transformer architecture replaces these recurrent/convolutional components entirely with **attention mechanisms**, allowing for significantly improved parallelization and better performance on many NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Embedding**\n",
    "Before feeding the input tokens to the Transformer, they are transformed into dense vector representations known as **embeddings**. An embedding maps each token (e.g., a word, subword, or character) to a fixed-dimensional continuous vector space.\n",
    "\n",
    "### **2.1 Brief Historical Perspective**\n",
    "Early approaches to represent words as numbers used **one-hot encoding**: each word in the vocabulary \\( V \\) is represented by a sparse vector with only one dimension set to 1 and the rest set to 0. This approach does not capture the semantic or syntactic relationships between words.\n",
    "\n",
    "Later, researchers developed **distributed representations** for words:\n",
    "- **Word2Vec (Mikolov et al., 2013)**: Learns embeddings by predicting context words from a target word (Skip-gram) or vice versa (CBOW).\n",
    "- **GloVe (Pennington et al., 2014)**: A model based on global word co-occurrence statistics.\n",
    "- **FastText (Bojanowski et al., 2017)**: Extends Word2Vec by representing words as n-grams of characters.\n",
    "\n",
    "The Transformer architecture paper (Vaswani et al., 2017) did not invent the concept of embeddings from scratch. Instead, it built upon this foundation. The key idea: each token in the vocabulary is associated with a learned dense vector, and this mapping is jointly trained with the rest of the Transformer.\n",
    "\n",
    "### **2.2 Building the Vocabulary**\n",
    "Transformers can work with different vocabulary construction methods:\n",
    "- **Word-based**: Assign each unique word an ID.\n",
    "- **Subword-based (BPE, WordPiece, etc.)**: Break words into smaller subword units.\n",
    "- **Character-based**: Each character is a token.\n",
    "\n",
    "Once the vocabulary is defined, each token \\( x_i \\) in a sequence is mapped to an index \\( \\text{index}(x_i) \\). If \\( E \\) is an embedding matrix of size \\( |V| \\times d_{model} \\), then\n",
    "\\$[\n",
    "e_i = E[\\text{index}(x_i)],\\quad i = 1, 2, ..., n.\n",
    "\\]$\n",
    "where$ \\( |V| \\)$ is the vocabulary size, and \\( d_{model} \\) is the dimension of each embedding vector.\n",
    "\n",
    "### **2.3 Embedding Mechanism in Transformers**\n",
    "The basic formula for the embedding lookup is straightforward:\n",
    "\\$[\n",
    "e_i = E[x_i],\n",
    "\\]$\n",
    "where \\( x_i \\) is the integer index of the token, and \\( E \\) is a learnable matrix. During training, backpropagation updates the rows of this embedding matrix so that semantically similar tokens end up in similar regions of the embedding space.\n",
    "\n",
    "### **2.4 Example Code for a Simple Embedding Layer**\n",
    "Below is a minimal PyTorch example that demonstrates how an embedding layer converts token indices to vectors. This is not the full Transformer code, just a snippet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded output shape: torch.Size([2, 5, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1922, -1.7240, -1.2398, -0.4538,  1.1732,  0.2561,  0.8341,\n",
       "           0.9263,  1.0329,  1.1626,  0.7192,  0.2055, -2.1686,  0.3992,\n",
       "          -1.4849, -0.0553],\n",
       "         [ 1.1031, -0.1748, -0.4883,  0.7280,  0.2793,  0.5590, -1.0705,\n",
       "           2.5811, -1.1439, -0.7423,  1.0178,  0.6332, -0.1173,  2.1692,\n",
       "          -0.6107,  0.5177],\n",
       "         [-1.2862, -0.5340,  0.2357, -0.6905,  1.7576,  0.2066,  1.8103,\n",
       "           0.7516, -0.8231, -0.1784,  1.3053, -0.2123,  1.1030, -0.8248,\n",
       "           0.1393, -0.4550],\n",
       "         [-0.4893,  0.8707,  0.0177, -0.0708,  0.5352,  1.1423,  2.9484,\n",
       "          -0.0720,  2.2152,  0.6408, -0.1529, -1.2833,  0.3310,  1.4489,\n",
       "          -0.1356, -0.9928],\n",
       "         [ 1.3111, -0.4251, -0.9976, -0.1813, -0.4673, -0.1273,  0.2141,\n",
       "           0.1439,  1.0661, -0.4623, -0.3320,  0.2234, -0.5272, -1.2586,\n",
       "           0.1332,  0.2646]],\n",
       "\n",
       "        [[ 1.0795, -0.2132,  1.4992, -1.4102, -0.5895, -0.4260, -1.2568,\n",
       "           1.2368,  1.2039,  1.5795,  3.7722, -1.4068, -0.9712, -1.9528,\n",
       "           0.3602,  1.0479],\n",
       "         [-0.2728,  1.9980,  0.5174, -0.0588,  0.5705,  0.4632,  0.6938,\n",
       "          -0.4348, -0.2106,  0.2315, -0.9929,  0.7393,  0.6667,  0.0110,\n",
       "           1.0496, -1.3098],\n",
       "         [ 0.0273,  0.0700, -0.3589, -1.7545,  1.9239, -1.9612,  0.1465,\n",
       "          -0.7309, -0.1788,  0.1019,  0.3229,  1.0804, -1.7957, -0.7629,\n",
       "           0.7348,  0.1390],\n",
       "         [ 0.6896,  0.2331, -0.0244,  0.6727, -1.7574,  1.5368, -1.1478,\n",
       "           0.3341, -2.2361,  0.0247,  0.0626,  1.6984,  0.4240, -0.5726,\n",
       "          -0.3593, -1.2377],\n",
       "         [ 0.4424, -0.4761,  0.7644, -1.3690,  1.1007, -0.5165,  0.4082,\n",
       "           0.7773,  1.5334,  0.0933,  0.7493, -1.3580, -0.8301,  1.1550,\n",
       "           0.7490, -0.2990]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suppose we have the following parameters\n",
    "vocab_size = 100  # just an example\n",
    "d_model = 16     # embedding dimension\n",
    "max_seq_len = 5  # assume we have 5 tokens in a sequence\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "# Example: a batch of 2 sequences, each of length 5\n",
    "batch_token_indices = torch.tensor([\n",
    "    [1, 5, 2, 99, 3],\n",
    "    [10, 11, 12, 13, 14]\n",
    "])\n",
    "\n",
    "# Pass through the embedding layer\n",
    "embedded_output = embedding_layer(batch_token_indices)\n",
    "print(\"Embedded output shape:\", embedded_output.shape)\n",
    "embedded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 Integrating Positional Encoding**\n",
    "As discussed, Transformers add a **positional encoding** to the embedding:\n",
    "\\[$\n",
    "z_i = e_i + PE_i,\\quad i = 1, 2, ..., n.\n",
    "$\\]\n",
    "where \\( PE_i \\) is typically computed via sinusoidal functions at different frequencies.\n",
    "\n",
    "---\n",
    "## **3. Positional Encoding**\n",
    "Transformers do not have a built-in notion of sequence order (like RNNs do). To address this, the model uses **positional encodings** to inject information about the relative or absolute position of tokens in the sequence.\n",
    "\n",
    "A common choice from the paper is to use sine and cosine functions of different frequencies:\n",
    "\\[$\n",
    "PE_{(pos,2i)} = \\sin\\Bigl(pos / 10^{4i/d_{model}}\\Bigr),\\quad\n",
    "PE_{(pos,2i+1)} = \\cos\\Bigl(pos / 10^{4i/d_{model}}\\Bigr),$\n",
    "\\]\n",
    "where\n",
    "- \\( pos \\) is the position in the sequence (starting from 0, 1, 2, ...),\n",
    "- \\( i \\) indexes the dimension,\n",
    "- \\( d_{model} \\) is the dimension of the embeddings.\n",
    "\n",
    "This way, each position has a unique positional encoding vector of dimension \\( d_{model} \\). The final input representation for each token is:\n",
    "\\[\n",
    "z_i = e_i + PE_i,\n",
    "\\]\n",
    "where \\( PE_i \\) is the positional encoding for position \\( i \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Scaled Dot-Product Attention**\n",
    "The core idea behind the Transformer is the **self-attention** mechanism. Self-attention computes a set of **queries** (Q), **keys** (K), and **values** (V) from the input to decide how much each token should pay attention to the other tokens in the sequence.\n",
    "\n",
    "### **4.1 Formulas**\n",
    "Given \\( Q \\), \\( K \\), and \\( V \\) each of dimension \\( d_k \\), the attention output is:\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl(\\frac{Q K^T}{\\sqrt{d_k}}\\Bigr) V.\n",
    "\\]\n",
    "- \\($ Q K^T $\\) produces a score matrix (how relevant each query is to each key).\n",
    "- We scale by \\( \\sqrt{d_k} \\) to prevent large values when \\( d_k \\) is large.\n",
    "- We apply the softmax function to ensure the attention weights sum to 1.\n",
    "- Finally, we use these weights to produce a weighted sum of the values \\( V \\).\n",
    "\n",
    "### **4.2 Code Implementation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output Shape: torch.Size([2, 5, 16])\n",
      "Attention Weights Shape: torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q, K, V are of shape: (batch_size, seq_len, d_k)\n",
    "    Returns attention output and attention weights.\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)  # dimension of K\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    return output, attn_weights\n",
    "\n",
    "# Example usage:\n",
    "batch_size, seq_len, d_k = 2, 5, 16\n",
    "Q = torch.rand(batch_size, seq_len, d_k)\n",
    "K = torch.rand(batch_size, seq_len, d_k)\n",
    "V = torch.rand(batch_size, seq_len, d_k)\n",
    "\n",
    "attn_output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Attention Output Shape:\", attn_output.shape)\n",
    "print(\"Attention Weights Shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Multi-Head Attention**\n",
    "Instead of computing a single attention function, the Transformer uses **multiple attention heads** to capture different aspects of the relationships between tokens.\n",
    "\n",
    "- We have \\( h \\) heads, each with parameters \\( W_Q^{(i)}, W_K^{(i)}, W_V^{(i)} \\) (linear transformations for Q, K, V for head \\( i \\)).\n",
    "- Each head produces an output, which we then concatenate and project again.\n",
    "\n",
    "### **5.1 Formula**\n",
    "\\[\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\n",
    "\\]\n",
    "where\n",
    "- \\( \\text{head}_i = \\text{Attention}(Q W_Q^{(i)}, K W_K^{(i)}, V W_V^{(i)}) \\).\n",
    "- \\( W^O \\) is a final linear transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Position-wise Feed-Forward Network (FFN)**\n",
    "After the attention layers, each position in the sequence passes through a fully connected feed-forward network (applied independently to each position). The typical form is:\n",
    "\\[\n",
    "\\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2.\n",
    "\\]\n",
    "This helps in adding non-linearity and effectively learning transformations for each position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Putting It All Together**\n",
    "A single **Transformer Encoder** layer consists of:\n",
    "1. Multi-head self-attention sublayer (with residual connection & layer normalization).\n",
    "2. Position-wise feed-forward network sublayer (with residual connection & layer normalization).\n",
    "\n",
    "The **Transformer Decoder** has a similar structure, with an additional cross-attention sublayer that attends to the encoder output.\n",
    "\n",
    "Overall, the key takeaway from *Attention Is All You Need* is that attention mechanisms alone, without recurrences or convolutions, can achieve state-of-the-art performance in sequence modeling tasks, especially in NLP.\n",
    "\n",
    "### **7.1 Advantages**\n",
    "- **Parallelization**: Unlike RNNs, the Transformer can process all tokens in a sequence at once.\n",
    "- **Long-range dependencies**: Self-attention does not degrade over long distances as quickly as RNN-based models.\n",
    "- **Modular design**: Easy to scale by increasing model depth, width, or number of heads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762). *Advances in Neural Information Processing Systems*, 30.\n",
    "- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781). *ICLR*.\n",
    "- Pennington, J., Socher, R., & Manning, C. (2014). [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162). *EMNLP*.\n",
    "- Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606). *TACL*.\n",
    "\n",
    "---\n",
    "**End of Notebook**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding 에 대해 조금 더 심화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer 임베딩 메커니즘: 상세 설명**\n",
    "\n",
    "**Transformer** 기반 모델에서 임베딩은 단어(혹은 서브워드) 같은 이산 토큰을 연속적인 벡터 표현으로 바꾸는 데 매우 중요한 역할을 합니다. 이 문서는 실제 단어를 숫자로 매핑하는 방법, **서브워드 임베딩**이 작동하는 방식, 그리고 **학습 가능한(Trainable) 임베딩**이 어떻게 학습되는지 자세히 설명합니다. 마지막으로, 1·2·3 단계(레벨)의 임베딩(문자, 서브워드, 단어) 간의 포함 관계에 대해서도 살펴봅니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 단어(혹은 서브워드) → 인덱스 매핑: 구체적인 예시\n",
    "\n",
    "### 1.1 간단한 단어 수준 매핑 (미국에서 유명한 동요)\n",
    "다음과 같은 매우 작은 어휘(vocabulary)를 가정해 봅시다:\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "&\\texttt{Vocabulary} = \\{\\langle \\text{PAD} \\rangle, \\langle \\text{UNK} \\rangle, \\text{the}, \\text{cat}, \\text{sat}, \\text{on}, \\text{mat}, \\text{dog}\\}\\\\\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "이를 다음처럼 정수 ID로 매핑할 수 있습니다:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "- 예를 들어 `\"the cat sat on the mat\"`이라는 문장은 `[2, 3, 4, 5, 2, 6]`이라는 정수 시퀀스로 변환됩니다.\r\n",
    "- 임베딩 계층(Embedding Layer)은 `(vocab_size, d_model)` 형태의 행렬 `E`를 가집니다.  \r\n",
    "  - 만약 `d_model = 8`이라면, 이 예시에서는 `E`의 크기가 `(8, 8)`이 됩니다.\r\n",
    "\r\n",
    "토큰 인덱스가 3(예: `\"cat\"`)이면, 임베딩 레이어는 행렬 `E`에서 3번째 행을 추출하여 길이가 8인 벡터를 얻게 됩니다.\r\n",
    "\r\n",
    "**하지만**, 실제 적용에서는 어휘가 매우 커지며 희귀 단어 때문에 **OOV(Out-Of-Vocabulary)** 문제가 발생해 단어 기반 어휘는 한계가 있습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 2. 서브워드 임베딩과 그 장점\r\n",
    "\r\n",
    "### 2.1 왜 서브워드 토크나이저를 사용할까?\r\n",
    "현대 Transformer 모델들은 **서브워드 토크나이저**(예: BPE, WordPiece)를 주로 사용합니다. 이유는 다음과 같습니다:\r\n",
    "1. **OOV 문제 최소화**: 전혀 본 적 없는 단어라도 서브워드 단위로 쪼개어 처리하면 `<UNK>`가 잘 발생하지 않습니다.\r\n",
    "2. **효율성**: 서브워드 어휘 크기는 (3만~5만 토큰 정도)로 모든 단어를 포함한 거대 어휘보다 상대적으로 작습니다.\r\n",
    "3. **일반화**: 희귀 단어나 신조어(예: `transformers`)도 서브워드로 분해할 수 있으므로, 모델이 학습한 부분 정보를 활용하여 의미를 어느 정도 학습할 수 있습니다.\r\n",
    "\r\n",
    "### 2.2 서브워드 매핑 예시\r\n",
    "가령 다음과 같은 (BPE나 WordPiece 스타일의) 서브워드 어휘가 있다고 합시다:\r\n",
    "yle) that contains:\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<PAD> -> 0 <UNK> -> 1 the -> 2 cat -> 3 sat -> 4 on -> 5 ma -> 6 ##t -> 7 dog -> 8 ... trans -> 50 ##for -> 51 ##mers -> 52 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제 서브워드 사전은 3만~5만 개 이상의 토큰을 포함할 수 있으며, 단어 조각(서브워드)들도 많습니다.\r\n",
    "- `\"mat\"`이라는 단어가 사전에 없을 경우, `[ma, ##t]`로 분할될 수 있습니다.\r\n",
    "- `\"transformers\"` 같이 긴 단어는 `[\"trans\", \"##for\", \"##mers\"]`처럼 분할될 수 있습니다.\r\n",
    "\r\n",
    "임베딩 레이어는 동일하게 각 서브워드 인덱스를 `[0, vocab_size-1]` 범위로 관리하며, 임베딩 행렬 `E`에서 해당 인덱스에 해당하는 벡터를 꺼내옵니다.\r\n",
    "\r\n",
    "### 2.3 신조어나 희귀 단어 처리\r\n",
    "- 서브워드를 사용하면 완전히 새로운 단어라도 부분 단위로 분해되어 `<UNK>`로 처리되는 것을 피할 수 있습니다.\r\n",
    "- 예를 들어 `\"transformers\"`라는 단어가 통째로 어휘에 없어도, `[\"trans\", \"##for\", \"##mers\"]`라는 서브워드를 각각 학습해두었다면 일부라도 인지하고 처리할 수 있습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 3. 학습 가능한 임베딩(Trainable Embedding): 알고리즘적 관점\r\n",
    "\r\n",
    "### 3.1 “학습 가능한” 임베딩이란?\r\n",
    "**학습 가능한 임베딩(Trainable Embedding) 행렬**은 파라미터 행렬 `E`이며, 다음과 같은 특성을 가집니다:\r\n",
    "1. 각 행(ROW)은 서브워드 인덱스와 대응됩니다.\r\n",
    "2. 행(Row) 수는 `vocab_size`와 같고, 열(Column) 수는 `d_model`입니다.\r\n",
    "3. 학습 과정에서 역전파(backpropagation)를 통해 `E`가 업데이트되어, 의미적으로나 문법적으로 관련 있는 토큰들이 임베딩 공간에서 가깝게 위치하도록 학습됩니다.\r\n",
    "\r\n",
    "Transformer를 대규모 말뭉치(corpus)로 학습할 때, 최종 출력 에러가 임베딩 레이어까지 역전파되어 각 서브워드 벡터 표현이 조금씩 조정됩니다.\r\n",
    "\r\n",
    "#### 구체적인 과정:\r\n",
    "1. 순전파(Forward pass):\r\n",
    "   - 토큰 ID \\\\( x_i \\\\)를 받아서 임베딩 벡터 \\\\( e_i = E[x_i] \\\\)를 취한다.\r\n",
    "   - 이 \\\\( e_i \\\\)가 어텐션, 피드포워드 등 여러 레이어를 거친다.\r\n",
    "2. 역전파(Backward pass):\r\n",
    "   - 예측 결과(pred)와 라벨(labels)을 비교해 손실(loss)을 계산한다 (예: 교차 엔트로피).\r\n",
    "   - \\\\( \\\\frac{\\\\partial \\\\text{Loss}}{\\\\partial e_i} \\\\)가 계산되어 임베딩 행렬 `E`의 대응하는 행(Row)에 대해 가중치(파라미(pseudo code)터)가 갱신된다.\r\n",
    "\r\n",
    "### 3.2 업데이트 의사코드 예시\r\n",
    "```python\r\n",
    "# Pseudocode 예시\r\n",
    "pred = TransformerModel(E[x])  # 임베딩 레이어로 토큰 x를 투입\r\n",
    "loss = compute_loss(pred, labels)\r\n",
    "loss.backward()  # backprop을 통해 E에도 gradient 계산\r\n",
    "optimizer.step() # E와 그 외 모델 가중치들을 업데이트\r\n",
    "tes E, as well as other model weights\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 1, 2, 3 임베딩: 계층적 관계\n",
    "문헌이나 실제 모델에 따라 여러 “레벨”의 임베딩을 언급하기도 합니다:\n",
    "\n",
    "문자(char) 단위 임베딩 (가장 하위 수준, 선택적)\n",
    "\n",
    "어떤 모델은 개별 문자를 임베딩합니다.\n",
    "형태소가 복잡한 언어이거나, 서브워드로도 해결하기 어려운 극단적인 사례에 쓰이곤 합니다.\n",
    "서브워드(subword) 임베딩 (Transformer에서 가장 흔히 사용)\n",
    "\n",
    "BPE, WordPiece, SentencePiece 등.\n",
    "하나의 “단어”가 여러 서브워드로 분할될 수 있습니다.\n",
    "단어(word) 임베딩\n",
    "\n",
    "전통적인 방식. 지금은 서브워드를 주로 쓰면서 많이 대체되었습니다.\n",
    "일부 모델은 문자 단위 임베딩 → 이를 합성해(subword 단위로) → 최종적으로 단어 임베딩을 형성하는 다단계 방식을 취할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
