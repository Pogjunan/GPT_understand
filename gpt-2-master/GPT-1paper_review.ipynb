{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72137291-88f5-40c5-a729-fb0cfc7e1284",
   "metadata": {},
   "source": [
    "# GPT-1 paper review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37b78f-0df1-43a0-a9e8-b10db637dc11",
   "metadata": {},
   "source": [
    "### `PAPER` :  `Improving Language Understanding by Generative Pre-Training` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e7866-bca7-46b9-bff9-20fa95e5741c",
   "metadata": {},
   "source": [
    "Transformer 디코더(Decoder)-기반 언어 모델을 **미리 학습(Pre-training)**한 뒤, 해당 모델을 활용해 다양한 NLP 태스크에 Fine-tuning하는 접근을 제안."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f984cf2-1d73-4191-bc67-bceb842c1b2d",
   "metadata": {},
   "source": [
    "### `What is ELmo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3199b1d-630d-4293-a315-932211d3e377",
   "metadata": {},
   "source": [
    "ELmo paper link : https://arxiv.org/pdf/1802.05365 quote 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfd6a4-929e-4edb-ae7a-3a05bab13da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2d822bf-96be-4ce9-b88e-013b49493d70",
   "metadata": {},
   "source": [
    "BiLSTM 기반의 **언어 모델(Language Model)** 을 사전학습하여 단어의 문맥적 벡터(Contextual Embedding)를 생성 <- GPT 이전의 embedding method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b1aae-06ba-4577-ae3c-2f10bbd71be0",
   "metadata": {},
   "source": [
    "### `What is ULMFiT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e729c-112f-4029-9917-81f774202a53",
   "metadata": {},
   "source": [
    "ULMFit paper link : https://arxiv.org/pdf/1801.06146\n",
    "`Universal Language Model Fine-tuning for Text Classification` : quote 5000 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94148df5-9151-4230-a64b-4efd88c7b067",
   "metadata": {},
   "source": [
    "LSTM 기반 언어 모델을 미리 학습한 뒤, 텍스트 분류 등에 활용할 때 Fine-tuning 전략(Freeze 기법, Gradual Unfreezing 등)을 제안하며\n",
    "GPT-1도 결국 Transformer 구조로 미리 학습한 언어 모델을 다양한 태스크에 응용한다는 점에서, ULMFiT와 유사한 아이디어사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf3523-7704-4fc2-9d90-b931cd2d06b7",
   "metadata": {},
   "source": [
    "### `What is BERT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a2443-109c-46d7-8970-9b3737e58091",
   "metadata": {},
   "source": [
    "BERT paper link : https://arxiv.org/pdf/1810.04805\n",
    "\n",
    "`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`\n",
    "\n",
    "quote 123000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad2056-0586-412f-ab82-d78fc374f90f",
   "metadata": {},
   "source": [
    "Transformer 인코더(Encoder) 기반의 양방향(Bidirectional) 언어 모델(Masked Language Model & Next Sentence Prediction)을 사전학습.\n",
    "GPT 계열(단방향/디코더)과 대조적으로 양방향성을 활용한다는 점에서 비교 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ebd1e-b952-485e-a718-f567c73b47c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a574d-c082-44bd-86cf-b1582ef58108",
   "metadata": {},
   "source": [
    "### `transfer learning 의 한계`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33352306-e2ca-46f8-9966-9d7a408a3fac",
   "metadata": {},
   "source": [
    "`이미지 처리(Computer Vision)` 분야에서는 `대규모 사전 학습(Pre-training) 모`델을 다양한 `하위 과제(Downstream Task)`에 활용하는 전략이 큰 성공을 거두었음.\n",
    "\n",
    "텍스트에 대한 대규모 언어 모델을 사전 학습(pre-training)하면, 텍스트 이해 능력을 학습할 수 있을 것이라는 점이 여러 연구로부터 시사되어 옴."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f1f68-b2d0-4fde-906a-f7f95098ac23",
   "metadata": {},
   "source": [
    "` Generative Language Model`을 대규모 텍스트 코퍼스(방대한 영어 텍스트)에 대해 선행 학습한 뒤, 이를 여러 NLP 태스크에 미세조정(Fine-tuning) 하는 전략을 제안함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44140c61-6189-425e-87e6-43f3e63af73a",
   "metadata": {},
   "source": [
    "**`transfer learning`** \n",
    "\n",
    "-> 이미지넷(ImageNet)이라는 대규모 데이터셋으로 CNN을 학습시킨 후, 그 가중치(Feature)를 고정 혹은 일부 미세조정(Fine-tuning)하여, 고양이/개 분류나 의료영상 분석 등의 다른 작은 이미지 문제에 활용\n",
    "\n",
    "\n",
    "**`pre-training`**\n",
    "\n",
    "-> 광범위하게 활용 , 이미지넷처럼 ‘라벨이 달린’ 대규모 데이터가 없는 경우가 많았고, 텍스트에 대한 전이(특히 문맥적 이해) 구현 방식이 명확치 않았음.\n",
    "\n",
    " “아주 큰 규모의unlabeled 데이터(= 코퍼스)를 이용해, 모델이 일반적인 ‘언어적 지식’을 먼저 학습하도록 하는 단계”를 말합니다.\n",
    "\n",
    " 이후, Downstream Task(감정 분석, 요약, 번역, 질의응답 등)를 할 때는, 이 미리 학습한 모델 가중치를 일부만 수정(미세조정, Fine-tuning) 하거나, 아예 “프롬프트(prompt)”만 던져서 모델이 학습된 지식을 활용하도록 만듭니다.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd588572-3504-4fec-93f9-4abc6ea73e33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49678a51-d216-464d-8d2f-ac25186a4c45",
   "metadata": {},
   "source": [
    " ## `GPT-1에서의 “전이학습 한계 극복” 요점 `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fb602-c447-4225-8c82-25757904b708",
   "metadata": {},
   "source": [
    "Transfer Learning의 한계” = 기존에 NLP 전이학습을 제대로 활용할 수 없던 점\n",
    "\n",
    "* (1) 지도 학습 데이터 부족,\n",
    "* (2) 초기 파라미터를 완전히 새로 학습해야 함\n",
    "* (3) 범용적 언어 이해 능력을 얻기 어려움\n",
    "\n",
    "해결책: `“Generative Pre-training(사전학습)”`\n",
    "\n",
    "방대한 코퍼스에서 언어 모델을 학습하여, 모델이 자연어 이해·생성 능력을 많이 확보하도록 함.\n",
    "이후 필요한 태스크별로 추가 데이터(라벨된 것)를 조금만 더 학습(미세조정)하면 높은 성능 달성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66a9dd-923c-471b-af54-5843b1236070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep)",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
