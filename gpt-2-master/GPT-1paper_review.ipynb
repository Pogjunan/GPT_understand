{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72137291-88f5-40c5-a729-fb0cfc7e1284",
   "metadata": {},
   "source": [
    "# GPT-1 paper review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37b78f-0df1-43a0-a9e8-b10db637dc11",
   "metadata": {},
   "source": [
    "### `PAPER` :  `Improving Language Understanding by Generative Pre-Training` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e7866-bca7-46b9-bff9-20fa95e5741c",
   "metadata": {},
   "source": [
    "Transformer 디코더(Decoder)-기반 언어 모델을 **미리 학습(Pre-training)**한 뒤, 해당 모델을 활용해 다양한 NLP 태스크에 Fine-tuning하는 접근을 제안."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f984cf2-1d73-4191-bc67-bceb842c1b2d",
   "metadata": {},
   "source": [
    "### `What is ELmo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3199b1d-630d-4293-a315-932211d3e377",
   "metadata": {},
   "source": [
    "ELmo paper link : https://arxiv.org/pdf/1802.05365 quote 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5197fdd-7349-4ac2-84c1-c9c2aa1cdeb6",
   "metadata": {},
   "source": [
    "ELmo is obsolete method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d822bf-96be-4ce9-b88e-013b49493d70",
   "metadata": {},
   "source": [
    "BiLSTM 기반의 **언어 모델(Language Model)** 을 사전학습하여 단어의 문맥적 벡터(Contextual Embedding)를 생성 <- GPT 이전의 embedding method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b1aae-06ba-4577-ae3c-2f10bbd71be0",
   "metadata": {},
   "source": [
    "### `What is ULMFiT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e729c-112f-4029-9917-81f774202a53",
   "metadata": {},
   "source": [
    "ULMFit paper link : https://arxiv.org/pdf/1801.06146\n",
    "`Universal Language Model Fine-tuning for Text Classification` : quote 5000 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94148df5-9151-4230-a64b-4efd88c7b067",
   "metadata": {},
   "source": [
    "LSTM 기반 언어 모델을 미리 학습한 뒤, 텍스트 분류 등에 활용할 때 Fine-tuning 전략(Freeze 기법, Gradual Unfreezing 등)을 제안하며\n",
    "GPT-1도 결국 Transformer 구조로 미리 학습한 언어 모델을 다양한 태스크에 응용한다는 점에서, ULMFiT와 유사한 아이디어사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3df02a-e876-4d54-a8bf-67ad51c4192c",
   "metadata": {},
   "source": [
    " ULMFiT is obsolete method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf3523-7704-4fc2-9d90-b931cd2d06b7",
   "metadata": {},
   "source": [
    "### `What is BERT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a2443-109c-46d7-8970-9b3737e58091",
   "metadata": {},
   "source": [
    "BERT paper link : https://arxiv.org/pdf/1810.04805\n",
    "\n",
    "`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`\n",
    "\n",
    "quote 123000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad2056-0586-412f-ab82-d78fc374f90f",
   "metadata": {},
   "source": [
    "Transformer 인코더(Encoder) 기반의 양방향(Bidirectional) 언어 모델(Masked Language Model & Next Sentence Prediction)을 사전학습.\n",
    "GPT 계열(단방향/디코더)과 대조적으로 양방향성을 활용한다는 점에서 비교 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ebd1e-b952-485e-a718-f567c73b47c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a574d-c082-44bd-86cf-b1582ef58108",
   "metadata": {},
   "source": [
    "### `transfer learning 의 한계`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33352306-e2ca-46f8-9966-9d7a408a3fac",
   "metadata": {},
   "source": [
    "`이미지 처리(Computer Vision)` 분야에서는 `대규모 사전 학습(Pre-training) 모`델을 다양한 `하위 과제(Downstream Task)`에 활용하는 전략이 큰 성공을 거두었음.\n",
    "\n",
    "텍스트에 대한 대규모 언어 모델을 사전 학습(pre-training)하면, 텍스트 이해 능력을 학습할 수 있을 것이라는 점이 여러 연구로부터 시사되어 옴."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f1f68-b2d0-4fde-906a-f7f95098ac23",
   "metadata": {},
   "source": [
    "` Generative Language Model`을 대규모 텍스트 코퍼스(방대한 영어 텍스트)에 대해 선행 학습한 뒤, 이를 여러 NLP 태스크에 미세조정(Fine-tuning) 하는 전략을 제안함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44140c61-6189-425e-87e6-43f3e63af73a",
   "metadata": {},
   "source": [
    "**`transfer learning`** \n",
    "\n",
    "-> 이미지넷(ImageNet)이라는 대규모 데이터셋으로 CNN을 학습시킨 후, 그 가중치(Feature)를 고정 혹은 일부 미세조정(Fine-tuning)하여, 고양이/개 분류나 의료영상 분석 등의 다른 작은 이미지 문제에 활용\n",
    "\n",
    "\n",
    "**`pre-training`**\n",
    "\n",
    "-> 광범위하게 활용 , 이미지넷처럼 ‘라벨이 달린’ 대규모 데이터가 없는 경우가 많았고, 텍스트에 대한 전이(특히 문맥적 이해) 구현 방식이 명확치 않았음.\n",
    "\n",
    " “아주 큰 규모의unlabeled 데이터(= 코퍼스)를 이용해, 모델이 일반적인 ‘언어적 지식’을 먼저 학습하도록 하는 단계”를 말합니다.\n",
    "\n",
    " 이후, Downstream Task(감정 분석, 요약, 번역, 질의응답 등)를 할 때는, 이 미리 학습한 모델 가중치를 일부만 수정(미세조정, Fine-tuning) 하거나, 아예 “프롬프트(prompt)”만 던져서 모델이 학습된 지식을 활용하도록 만듭니다.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd588572-3504-4fec-93f9-4abc6ea73e33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49678a51-d216-464d-8d2f-ac25186a4c45",
   "metadata": {},
   "source": [
    " ## `GPT-1에서의 “전이학습 한계 극복” 요점 `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fb602-c447-4225-8c82-25757904b708",
   "metadata": {},
   "source": [
    "Transfer Learning의 한계” = 기존에 NLP 전이학습을 제대로 활용할 수 없던 점\n",
    "\n",
    "* (1) 지도 학습 데이터 부족,\n",
    "* (2) 초기 파라미터를 완전히 새로 학습해야 함\n",
    "* (3) 범용적 언어 이해 능력을 얻기 어려움\n",
    "\n",
    "해결책: `“Generative Pre-training(사전학습)”`\n",
    "\n",
    "방대한 코퍼스에서 언어 모델을 학습하여, 모델이 자연어 이해·생성 능력을 많이 확보하도록 함.\n",
    "이후 필요한 태스크별로 추가 데이터(라벨된 것)를 조금만 더 학습(미세조정)하면 높은 성능 달성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7696e1-cd47-44f3-8eae-231504b0ed3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f57f6-c29f-4fe0-804e-d403f167f8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b537bcd-8460-403b-8bd7-fd011a975b49",
   "metadata": {},
   "source": [
    "\n",
    "# Generative Pre-training (사전학습) 상세 설명\n",
    "\n",
    "**Generative Pre-training(사전학습)**은 대규모 비라벨 텍스트 데이터를 활용하여 언어 모델을 먼저 학습시키고, 이를 다양한 자연어 처리(NLP) 태스크에 전이(Transfer)하여 성능을 향상시키는 접근 방식입니다. 이 과정은 크게 **사전학습(Pre-training)**과 **미세조정(Fine-tuning)** 단계로 나눌 수 있으며, 각 단계에서의 작동 원리와 수식적 표현을 상세히 다루겠습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 사전학습(Pre-training)의 목적과 개념\n",
    "\n",
    "### 1.1 목적\n",
    "- **전이학습(Transfer Learning)의 효율성 향상**: 사전학습을 통해 모델이 일반적인 언어적 지식(어휘, 문법, 문맥 등)을 습득하게 하여, 이후 다양한 태스크에 적은 양의 데이터로도 높은 성능을 발휘할 수 있도록 함.\n",
    "- **라벨 데이터 부족 문제 해결**: 비라벨 데이터인 대규모 텍스트 코퍼스를 활용하여 모델을 학습함으로써, 라벨 데이터가 부족한 상황에서도 효과적으로 모델을 적용할 수 있음.\n",
    "\n",
    "### 1.2 개념\n",
    "- **Generative Language Model**: 주어진 문맥을 바탕으로 다음 단어를 예측하는 모델. GPT-1은 Transformer의 디코더 구조를 기반으로 단방향(Left-to-Right) 언어 모델을 구현함.\n",
    "- **자기지도 학습(Self-supervised Learning)**: 라벨이 없는 데이터를 활용하여 모델을 학습시키는 방법. 다음 단어 예측은 입력 데이터에서 일부를 예측하는 자기지도 학습의 한 형태임.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 사전학습의 수식적 표현\n",
    "\n",
    "### 2.1 언어 모델의 목표 함수\n",
    "\n",
    "Generative Pre-training에서 사용되는 언어 모델은 **다음 단어 예측**을 목표로 합니다. 이를 수식으로 표현하면 다음과 같습니다.\n",
    "$$\\mathcal{L}(\\theta) = - \\sum_{t=1}^{T} \\log P_\\theta(w_t \\mid w_1, w_2, \\dots, w_{t-1})\n",
    "$$\n",
    "- $ (\\theta) $: 모델의 파라미터\n",
    "- $(w_t)$: 시퀀스의 \\(t\\)번째 단어\n",
    "- $(T)$: 시퀀스의 총 단어 수\n",
    "- $ (P_\\theta(w_t \\mid w_1, w_2, \\dots, w_{t-1})) $: 모델이 예측한 \\(t\\)번째 단어의 확률\n",
    "\n",
    "### 2.2 Transformer 디코더 구조\n",
    "\n",
    "GPT-1은 Transformer의 디코더를 기반으로 합니다. Transformer 디코더의 주요 구성 요소는 **Self-Attention**과 **Feed-Forward Neural Network**입니다. 각 디코더 레이어는 다음과 같은 연산을 수행합니다.\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_l = \\text{Self-Attention}(\\mathbf{z}_{l-1}) + \\mathbf{z}_{l-1}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{z}_l = \\text{Feed-Forward}(\\mathbf{z}_l) + \\mathbf{z}_l\n",
    "$$\n",
    "\n",
    "- **$(\\mathbf{z}_l)$**: \\(l\\)번째 레이어의 출력\n",
    "- **Self-Attention**: 입력 시퀀스 내의 모든 단어 간의 상호작용을 계산\n",
    "- **Feed-Forward**: 각 단어별로 독립적으로 적용되는 신경망\n",
    "\n",
    "### 2.3 손실 함수 최적화\n",
    "\n",
    "사전학습 단계에서는 언어 모델의 손실 함수를 최소화하기 위해 **확률적 경사 하강법(Stochastic Gradient Descent, SGD)** 등을 사용하여 파라미터 $\\theta$를 최적화합니다.\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min_\\theta \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 사전학습 단계의 구체적 과정\n",
    "\n",
    "### 3.1 데이터 준비\n",
    "- **코퍼스(Corpus)**: 대규모 비라벨 텍스트 데이터(예: BookCorpus, 위키피디아, 웹 텍스트 등)를 수집.\n",
    "- **토크나이제이션(Tokenization)**: 텍스트를 토큰(단어 혹은 서브워드 단위)으로 분할하고, 각 토큰을 고유한 정수 인덱스로 매핑.\n",
    "\n",
    "### 3.2 입력 표현\n",
    "- **단어 임베딩(Word Embedding)**: 각 토큰 \\(w_t\\)를 고차원 벡터 \\(\\mathbf{e}_t\\)로 변환.\n",
    "- **포지셔널 인코딩(Positional Encoding)**: 단어의 순서 정보를 추가하기 위해 포지셔널 인코딩 벡터 \\(\\mathbf{p}_t\\)를 더함.\n",
    "  \n",
    "$$\n",
    "\\mathbf{z}_t = \\mathbf{e}_t + \\mathbf{p}_t\n",
    "$$\n",
    "\n",
    "### 3.3 모델 학습\n",
    "- Transformer 디코더를 통해 입력 시퀀스 \\(\\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_T\\)를 처리.\n",
    "- 각 단어 \\(w_t\\)에 대해 다음 단어를 예측하도록 모델을 학습.\n",
    "- 손실 함수 $(\\mathcal{L}(\\theta))$를 최소화하도록 파라미터 \\(\\theta\\)를 업데이트.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 미세조정(Fine-tuning) 단계\n",
    "\n",
    "### 4.1 목적\n",
    "- 사전학습된 언어 모델을 특정 다운스트림 태스크(예: 감정 분석, 질의응답, 번역 등)에 맞게 조정하여 성능을 향상시킴.\n",
    "\n",
    "### 4.2 과정\n",
    "1. **태스크별 데이터 준비**: 라벨이 있는 데이터셋을 준비.\n",
    "2. **모델 확장**: 필요한 경우, 태스크에 특화된 출력 레이어(예: 분류를 위한 소프트맥스 레이어)를 추가.\n",
    "3. **미세조정**: 사전학습된 파라미터 \\(\\theta\\)를 초기화 상태로 사용하고, 태스크별 손실 함수를 최소화하도록 추가 학습.\n",
    "  \n",
    "$$\n",
    "\\theta_{\\text{fine-tuned}} = \\arg\\min_{\\theta} \\mathcal{L}_{\\text{task}}(\\theta)\n",
    "$$\n",
    "\n",
    "- **$(\\mathcal{L}_{\\text{task}}(\\theta))$**: 특정 태스크에 대한 손실 함수\n",
    "\n",
    "### 4.3 수식적 예시: 텍스트 분류 태스크\n",
    "\n",
    "텍스트 분류 태스크에서의 손실 함수는 보통 크로스 엔트로피 손실을 사용합니다.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{task}}(\\theta) = - \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log \\hat{y}_{i,c}\n",
    "$$\n",
    "\n",
    "- **$(N)$**: 샘플 수\n",
    "- **$(C)$**: 클래스 수\n",
    "- **$(y_{i,c})$**: 샘플 \\(i\\)가 클래스 \\(c\\)에 속할 때 1, 아니면 0\n",
    "- **$(\\hat{y}_{i,c})$**: 모델이 예측한 샘플 \\(i\\)의 클래스 \\(c\\)에 대한 확률\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Generative Pre-training의 장점과 효과\n",
    "\n",
    "### 5.1 장점\n",
    "- **범용성**: 사전학습된 모델은 다양한 태스크에 전이 가능.\n",
    "- **효율성**: 미세조정 시 적은 양의 라벨 데이터로도 높은 성능 달성.\n",
    "- **학습 안정성**: 대규모 코퍼스를 통한 학습으로 일반적인 언어적 패턴을 잘 학습.\n",
    "\n",
    "### 5.2 효과\n",
    "- **성능 향상**: 사전학습을 통해 모델이 기본적인 언어 이해 능력을 갖추어, 다운스트림 태스크에서 뛰어난 성능을 보임.\n",
    "- **학습 속도 향상**: 이미 학습된 파라미터를 활용하므로, 미세조정 단계에서 학습 속도가 빨라짐.\n",
    "- **데이터 효율성**: 라벨이 없는 대규모 데이터 활용 가능, 라벨 데이터의 필요성을 줄임.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 수식적 요약\n",
    "\n",
    "1. **사전학습 손실 함수**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{pre-training}}(\\theta) = - \\sum_{t=1}^{T} \\log P_\\theta(w_t \\mid w_1, w_2, \\dots, w_{t-1})\n",
    "$$\n",
    "\n",
    "2. **미세조정 손실 함수 (예: 텍스트 분류)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{fine-tuning}}(\\theta) = - \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log \\hat{y}_{i,c}\n",
    "$$\n",
    "\n",
    "3. **모델 파라미터 최적화**:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min_\\theta \\left( \\mathcal{L}_{\\text{pre-training}}(\\theta) + \\mathcal{L}_{\\text{fine-tuning}}(\\theta) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 결론\n",
    "\n",
    "**Generative Pre-training(사전학습)**은 대규모 비라벨 텍스트 코퍼스를 활용하여 언어 모델을 먼저 학습시키고, 이를 다양한 NLP 태스크에 전이하여 높은 성능을 달성하는 효과적인 접근 방식입니다. 수식적으로는 다음 단어 예측을 목표로 하는 손실 함수를 최소화하는 과정과, 특정 태스크에 맞게 미세조정하는 과정을 통해 모델의 범용성과 효율성을 극대화합니다. GPT-1은 이러한 사전학습-전이학습 패러다임을 성공적으로 구현한 대표적인 예로, 이후의 GPT-2, GPT-3 등의 발전에 큰 영향을 미쳤습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5061c4c6-ffd1-4599-b781-6b9ef38b8d2c",
   "metadata": {},
   "source": [
    "### GPT-1의 혁신적인 측면\n",
    "단일 모델 접근법: 사전 학습과 미세 조정을 하나의 통합된 프레임워크 내에서 수행하여 다양한 NLP 작업에 유연하게 대응할 수 있습니다.\n",
    "\n",
    "\n",
    "규모의 경제: 모델의 크기와 데이터의 양을 증가시킴으로써 성능 향상을 도모하였습니다.\n",
    "\n",
    " \n",
    "전이 학습의 가능성: 사전 학습된 모델을 다양한 도메인과 작업에 쉽게 적용할 수 있는 기반을 마련하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4764b8-1a8e-4b1f-96ff-ac739638230d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep)",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
