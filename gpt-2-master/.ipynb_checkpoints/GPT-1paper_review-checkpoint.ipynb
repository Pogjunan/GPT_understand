{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72137291-88f5-40c5-a729-fb0cfc7e1284",
   "metadata": {},
   "source": [
    "# GPT-1 paper review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37b78f-0df1-43a0-a9e8-b10db637dc11",
   "metadata": {},
   "source": [
    "### `PAPER` :  `Improving Language Understanding by Generative Pre-Training` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e7866-bca7-46b9-bff9-20fa95e5741c",
   "metadata": {},
   "source": [
    "Transformer 디코더(Decoder)-기반 언어 모델을 **미리 학습(Pre-training)**한 뒤, 해당 모델을 활용해 다양한 NLP 태스크에 Fine-tuning하는 접근을 제안."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f984cf2-1d73-4191-bc67-bceb842c1b2d",
   "metadata": {},
   "source": [
    "### `What is ELmo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3199b1d-630d-4293-a315-932211d3e377",
   "metadata": {},
   "source": [
    "ELmo paper link : https://arxiv.org/pdf/1802.05365 quote 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfd6a4-929e-4edb-ae7a-3a05bab13da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2d822bf-96be-4ce9-b88e-013b49493d70",
   "metadata": {},
   "source": [
    "BiLSTM 기반의 **언어 모델(Language Model)** 을 사전학습하여 단어의 문맥적 벡터(Contextual Embedding)를 생성 <- GPT 이전의 embedding method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b1aae-06ba-4577-ae3c-2f10bbd71be0",
   "metadata": {},
   "source": [
    "### `What is ULMFiT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e729c-112f-4029-9917-81f774202a53",
   "metadata": {},
   "source": [
    "ULMFit paper link : https://arxiv.org/pdf/1801.06146\n",
    "`Universal Language Model Fine-tuning for Text Classification` : quote 5000 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94148df5-9151-4230-a64b-4efd88c7b067",
   "metadata": {},
   "source": [
    "LSTM 기반 언어 모델을 미리 학습한 뒤, 텍스트 분류 등에 활용할 때 Fine-tuning 전략(Freeze 기법, Gradual Unfreezing 등)을 제안하며\n",
    "GPT-1도 결국 Transformer 구조로 미리 학습한 언어 모델을 다양한 태스크에 응용한다는 점에서, ULMFiT와 유사한 아이디어사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf3523-7704-4fc2-9d90-b931cd2d06b7",
   "metadata": {},
   "source": [
    "### `What is BERT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a2443-109c-46d7-8970-9b3737e58091",
   "metadata": {},
   "source": [
    "BERT paper link : https://arxiv.org/pdf/1810.04805\n",
    "\n",
    "`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`\n",
    "\n",
    "quote 123000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad2056-0586-412f-ab82-d78fc374f90f",
   "metadata": {},
   "source": [
    "Transformer 인코더(Encoder) 기반의 양방향(Bidirectional) 언어 모델(Masked Language Model & Next Sentence Prediction)을 사전학습.\n",
    "GPT 계열(단방향/디코더)과 대조적으로 양방향성을 활용한다는 점에서 비교 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a574d-c082-44bd-86cf-b1582ef58108",
   "metadata": {},
   "source": [
    "### `transfer learning 의 한계`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33352306-e2ca-46f8-9966-9d7a408a3fac",
   "metadata": {},
   "source": [
    "`이미지 처리(Computer Vision)` 분야에서는 `대규모 사전 학습(Pre-training) 모`델을 다양한 `하위 과제(Downstream Task)`에 활용하는 전략이 큰 성공을 거두었음.\n",
    "\n",
    "텍스트에 대한 대규모 언어 모델을 사전 학습(pre-training)하면, 텍스트 이해 능력을 학습할 수 있을 것이라는 점이 여러 연구로부터 시사되어 옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0609c-52a2-4de7-97ee-3c9e09dcdd31",
   "metadata": {},
   "outputs": [],
   "source": [
    " Generative Language Model을 대규모 텍스트 코퍼스에 대해 선행 학습한 뒤, 이를 여러 NLP 태스크에 미세조정(Fine-tuning) 하는 전략을 제안함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep)",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
