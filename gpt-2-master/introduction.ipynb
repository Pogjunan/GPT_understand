{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def40ab2-a1fb-4d0f-ae8b-5e35e7c3b1b5",
   "metadata": {},
   "source": [
    "# GPT-2 from GPT-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce7d7b-6266-4209-adf1-30d57adb6299",
   "metadata": {},
   "source": [
    "we know how to operate `GPT-1`\n",
    "\n",
    "`Transformer` - `??`  - `GPT-1` - `GPT-2` - .. - `GPT-o3`\n",
    "I think to need filling `??` , so we have to learn from fromer papers.\n",
    "\n",
    "At least I understood Transformer paper during 2 month ,when i was 3 grad of statistics major and started self-teaching deeplearning during 1years.\n",
    "so today we have to leap next step which help to understand GPT-3 & GPT-o1 & GPT-o3..\n",
    "\n",
    "I surprised gpt-o3 get AGI so I can't delay any longer. study hard to understand and get yours. Develop and evolve based on what you have learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65744d56-52fe-4c65-bb78-a0fec4ba6a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3cb53bc-639d-4d7a-8f24-d1ccc3d2084f",
   "metadata": {},
   "source": [
    "# `Transformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87784be-1d71-404c-ac78-b9e81770c182",
   "metadata": {},
   "source": [
    "`Attention Is All You Need`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfc18d-564a-4b46-8008-010088e355a3",
   "metadata": {},
   "source": [
    "`Abstract`\n",
    "\n",
    "`Introduction`\n",
    "\n",
    "`Background`\n",
    "\n",
    "`Model Architecture`\n",
    "\n",
    "`Encoder and Decoder Stacks`\n",
    "\n",
    "`Attention`\n",
    "\n",
    "`Scaled Dot-Product Attention`\n",
    "\n",
    "`Multi-Head Attention`\n",
    "\n",
    "`Applications of Attention in our Model`\n",
    "\n",
    "`Position-wise Feed-Forward Networks`\n",
    "\n",
    "`Embeddings and Softmax`\n",
    "\n",
    "`Positional Encoding`\n",
    "\n",
    "`Why Self-Attention`\n",
    "\n",
    "\n",
    "` Training`\n",
    "\n",
    "`Optimizer`\n",
    "\n",
    "`Regularization`\n",
    "\n",
    "\n",
    "`Results`\n",
    "`Machine Translation`\n",
    "`English Constituency Parsing`\n",
    "\n",
    "` Conclusion`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bae574-62ba-4cd4-94d5-be3791b9d596",
   "metadata": {},
   "source": [
    "# Embedding techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f19392-d667-40ac-af3d-2ee39968552b",
   "metadata": {},
   "source": [
    "## Word Embedding(단어 임베딩)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4752b2-6590-4448-bc29-7aef540f32c2",
   "metadata": {},
   "source": [
    "# Transformer Embedding Techniques\n",
    "\n",
    "This document provides a concise yet comprehensive overview of the embedding techniques introduced in the original Transformer paper, \"Attention Is All You Need\" (Vaswani et al.). We will focus on two main components:\n",
    "\n",
    "1. **Word Embedding**\n",
    "2. **Positional Encoding**\n",
    "\n",
    "Both components combine to form the input representation for the Transformer, enabling the model to effectively process the meaning and order of tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Word Embedding\n",
    "\n",
    "### 1.1 Concept\n",
    "In the Transformer, an input sentence (token sequence) is first converted into a sequence of integers $\\ (\\{x_1, x_2, \\dots, x_n\\}\\ )$. Each token $\\(x_i\\)  $ is mapped to a vector $ \\(\\mathbf{e}_i \\in \\mathbb{R}^{d_{\\text{model}}}\\) $ using an embedding matrix $\\(\\mathbf{E}\\)  $ of size $\\(\\text{vocab\\_size} \\times d_{\\text{model}}\\) $ . Formally:\n",
    "$\n",
    "\\[\n",
    "\\mathbf{E}(x_i) = \\mathbf{e}_i \\quad \\in \\mathbb{R}^{d_{\\text{model}}}.\n",
    "\\]\n",
    "$\n",
    "These embedding vectors capture semantic or distributional features of each token.\n",
    "\n",
    "### 1.2 Scaling\n",
    "The Transformer paper proposes multiplying the embedding by \\(\\sqrt{d_{\\text{model}}}\\) to adjust the variance of the embeddings when they enter the model. This scaling helps:\n",
    "\n",
    "- Align the variance with the dimensionality \\(d_{\\text{model}}\\).  \n",
    "- Maintain an appropriate signal scale through residual connections.\n",
    "\n",
    "Hence, the initial input to the model often becomes:\n",
    "\n",
    "\\[\n",
    "\\mathbf{z}_i = \\sqrt{d_{\\text{model}}}\\,\\mathbf{e}_i + \\mathbf{P}_i,\n",
    "\\]\n",
    "\n",
    "where \\(\\mathbf{P}_i\\) is the positional encoding vector (explained below).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Positional Encoding\n",
    "\n",
    "### 2.1 Background\n",
    "Unlike RNNs (e.g., LSTM, GRU), the Transformer processes all tokens in parallel via **Self-Attention** without an inherent notion of sequential ordering. Therefore, the model needs an external mechanism to inject **positional (sequence) information**. This is achieved through **Positional Encoding**.\n",
    "\n",
    "### 2.2 Sinusoidal Encoding\n",
    "The original paper uses sinusoidal functions (sine and cosine) to create position-dependent vectors. For each position \\(pos\\) and embedding dimension index \\(i\\):\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "PE_{(pos, 2i)} &= \\sin\\left(pos \\cdot \\frac{1}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), \\\\\n",
    "PE_{(pos, 2i+1)} &= \\cos\\left(pos \\cdot \\frac{1}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right).\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "- \\(pos\\) is the token index in the sequence, \\(0 \\le pos \\le n-1\\).  \n",
    "- \\(i\\) is the embedding dimension index.  \n",
    "- \\(d_{\\text{model}}\\) is the model’s hidden dimension.\n",
    "\n",
    "This design allows adjacent positions to have similar encodings, while distant positions produce quite different vectors due to the multiple frequencies (periods) involved.\n",
    "\n",
    "### 2.3 Intuition\n",
    "- **Continuous positional signals** are encoded as sinusoidal waveforms with varying frequencies.\n",
    "- By leveraging these encodings, the attention mechanism can learn both **absolute** and **relative** positional relationships among tokens.\n",
    "\n",
    "### 2.4 Learned Positional Encoding\n",
    "While the paper uses fixed sinusoidal encodings, later studies have explored **learned** positional embeddings:\n",
    "\n",
    "- A learned positional embedding approach has a set of parameters \\(\\mathbf{W}_{pos}\\in \\mathbb{R}^{\\text{max\\_seq\\_length} \\times d_{\\text{model}}}\\) that the model optimizes during training.  \n",
    "- Sinusoidal embeddings, however, generalize more naturally to sequences longer than those seen in training (extrapolation).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Overall Flow & Implementation Considerations\n",
    "\n",
    "### 3.1 Encoder Input Pipeline\n",
    "1. Convert the integer sequence \\(\\{x_1, x_2, \\dots, x_n\\}\\) into embeddings \\(\\mathbf{e}_i\\) via the embedding matrix \\(\\mathbf{E}\\).\n",
    "2. Multiply \\(\\mathbf{e}_i\\) by \\(\\sqrt{d_{\\text{model}}}\\) to rescale.\n",
    "3. Add the positional encoding vector \\(\\mathbf{P}_i\\) elementwise.\n",
    "4. Optionally apply **Dropout**, then feed the result to **Multi-Head Attention** blocks and subsequent layers.\n",
    "\n",
    "### 3.2 Decoder-Side Processing\n",
    "- The decoder applies the same embedding + positional encoding process to its input tokens.  \n",
    "- **Masked Multi-Head Attention** ensures the decoder cannot access future tokens.\n",
    "- Output embeddings are often tied with the input embedding parameters for efficiency and better generalization.\n",
    "\n",
    "### 3.3 Embedding Weight Tying\n",
    "In the paper, **weight tying** is recommended to reduce the total number of parameters and improve generalization:\n",
    "\n",
    "\\[\n",
    "P(\\text{word} \\mid \\mathbf{z}) = \\text{softmax}\\bigl(\\mathbf{W}_{\\text{softmax}}^T \\mathbf{z}\\bigr),\n",
    "\\]\n",
    "\n",
    "where \\(\\mathbf{W}_{\\text{softmax}}\\) can be the same matrix as the embedding matrix \\(\\mathbf{E}\\) (with dimension alignment).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Key Takeaways\n",
    "\n",
    "1. **Word Embeddings**  \n",
    "   - Provide high-dimensional representations of tokens.  \n",
    "   - In Transformers, scaled by \\(\\sqrt{d_{\\text{model}}}\\) to maintain consistent variance for residual connections.\n",
    "\n",
    "2. **Positional Encoding**  \n",
    "   - Essential for order information in a parallel Self-Attention architecture.  \n",
    "   - Sinusoidal (fixed) vs. learned approaches each have pros and cons.\n",
    "\n",
    "3. **Embedding Weight Tying**  \n",
    "   - Reduces parameter count by sharing weights between input embeddings and output softmax weights.\n",
    "\n",
    "4. **Practical Tips**  \n",
    "   - Pre-compute the sinusoidal positional embedding table (up to a maximum length) and index into it.  \n",
    "   - Carefully integrate **Dropout** and **Layer Normalization** layers per the original paper’s guidelines.  \n",
    "   - Use appropriate initialization strategies (e.g., Xavier) for stable training.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "In the Transformer architecture, the combination of **word embeddings** and **positional encodings** ensures both semantic and positional information are accessible to the Self-Attention mechanism. For a faithful re-implementation as described in the paper, pay particular attention to details like:\n",
    "\n",
    "- Embedding scaling (\\(\\sqrt{d_{\\text{model}}}\\))\n",
    "- Sinusoidal vs. learned positional encodings\n",
    "- Weight tying to reduce parameters\n",
    "- Proper placement of Dropout and LayerNorm\n",
    "\n",
    "By carefully incorporating these aspects, one can leverage the full potential of the Transformer’s parallel attention design for sequence modeling tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d09393-4072-41f8-8407-588158b58b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep)",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
