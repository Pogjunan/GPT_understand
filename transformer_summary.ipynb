{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer (Attention Is All You Need) Overview**\n",
    "\n",
    "This notebook provides a concise summary of the seminal paper \"[Attention is All You Need (2017)](https://arxiv.org/abs/1706.03762)\" by Vaswani et al., which introduced the Transformer architecture. The goal is to explain the core concepts and mathematical formulations (in English) without any images.\n",
    "\n",
    "## **1. Introduction**\n",
    "Traditional sequence-to-sequence models relied on recurrent or convolutional structures, which made it difficult to process sequences in parallel and capture long-range dependencies efficiently. The Transformer architecture replaces these recurrent/convolutional components entirely with **attention mechanisms**, allowing for significantly improved parallelization and better performance on many NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Embedding**\n",
    "Before feeding the input tokens to the Transformer, they are transformed into dense vector representations known as **embeddings**. An embedding maps each token (e.g., a word, subword, or character) to a fixed-dimensional continuous vector space.\n",
    "\n",
    "### **2.1 Brief Historical Perspective**\n",
    "Early approaches to represent words as numbers used **one-hot encoding**: each word in the vocabulary \\( V \\) is represented by a sparse vector with only one dimension set to 1 and the rest set to 0. This approach does not capture the semantic or syntactic relationships between words.\n",
    "\n",
    "Later, researchers developed **distributed representations** for words:\n",
    "- **Word2Vec (Mikolov et al., 2013)**: Learns embeddings by predicting context words from a target word (Skip-gram) or vice versa (CBOW).\n",
    "- **GloVe (Pennington et al., 2014)**: A model based on global word co-occurrence statistics.\n",
    "- **FastText (Bojanowski et al., 2017)**: Extends Word2Vec by representing words as n-grams of characters.\n",
    "\n",
    "The Transformer architecture paper (Vaswani et al., 2017) did not invent the concept of embeddings from scratch. Instead, it built upon this foundation. The key idea: each token in the vocabulary is associated with a learned dense vector, and this mapping is jointly trained with the rest of the Transformer.\n",
    "\n",
    "### **2.2 Building the Vocabulary**\n",
    "Transformers can work with different vocabulary construction methods:\n",
    "- **Word-based**: Assign each unique word an ID.\n",
    "- **Subword-based (BPE, WordPiece, etc.)**: Break words into smaller subword units.\n",
    "- **Character-based**: Each character is a token.\n",
    "\n",
    "Once the vocabulary is defined, each token \\( x_i \\) in a sequence is mapped to an index \\( \\text{index}(x_i) \\). If \\( E \\) is an embedding matrix of size \\( |V| \\times d_{model} \\), then\n",
    "\\[\n",
    "e_i = E[\\text{index}(x_i)],\\quad i = 1, 2, ..., n.\n",
    "\\]\n",
    "where \\( |V| \\) is the vocabulary size, and \\( d_{model} \\) is the dimension of each embedding vector.\n",
    "\n",
    "### **2.3 Embedding Mechanism in Transformers**\n",
    "The basic formula for the embedding lookup is straightforward:\n",
    "\\[\n",
    "e_i = E[x_i],\n",
    "\\]\n",
    "where \\( x_i \\) is the integer index of the token, and \\( E \\) is a learnable matrix. During training, backpropagation updates the rows of this embedding matrix so that semantically similar tokens end up in similar regions of the embedding space.\n",
    "\n",
    "### **2.4 Example Code for a Simple Embedding Layer**\n",
    "Below is a minimal PyTorch example that demonstrates how an embedding layer converts token indices to vectors. This is not the full Transformer code, just a snippet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded output shape: torch.Size([2, 5, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1922, -1.7240, -1.2398, -0.4538,  1.1732,  0.2561,  0.8341,\n",
       "           0.9263,  1.0329,  1.1626,  0.7192,  0.2055, -2.1686,  0.3992,\n",
       "          -1.4849, -0.0553],\n",
       "         [ 1.1031, -0.1748, -0.4883,  0.7280,  0.2793,  0.5590, -1.0705,\n",
       "           2.5811, -1.1439, -0.7423,  1.0178,  0.6332, -0.1173,  2.1692,\n",
       "          -0.6107,  0.5177],\n",
       "         [-1.2862, -0.5340,  0.2357, -0.6905,  1.7576,  0.2066,  1.8103,\n",
       "           0.7516, -0.8231, -0.1784,  1.3053, -0.2123,  1.1030, -0.8248,\n",
       "           0.1393, -0.4550],\n",
       "         [-0.4893,  0.8707,  0.0177, -0.0708,  0.5352,  1.1423,  2.9484,\n",
       "          -0.0720,  2.2152,  0.6408, -0.1529, -1.2833,  0.3310,  1.4489,\n",
       "          -0.1356, -0.9928],\n",
       "         [ 1.3111, -0.4251, -0.9976, -0.1813, -0.4673, -0.1273,  0.2141,\n",
       "           0.1439,  1.0661, -0.4623, -0.3320,  0.2234, -0.5272, -1.2586,\n",
       "           0.1332,  0.2646]],\n",
       "\n",
       "        [[ 1.0795, -0.2132,  1.4992, -1.4102, -0.5895, -0.4260, -1.2568,\n",
       "           1.2368,  1.2039,  1.5795,  3.7722, -1.4068, -0.9712, -1.9528,\n",
       "           0.3602,  1.0479],\n",
       "         [-0.2728,  1.9980,  0.5174, -0.0588,  0.5705,  0.4632,  0.6938,\n",
       "          -0.4348, -0.2106,  0.2315, -0.9929,  0.7393,  0.6667,  0.0110,\n",
       "           1.0496, -1.3098],\n",
       "         [ 0.0273,  0.0700, -0.3589, -1.7545,  1.9239, -1.9612,  0.1465,\n",
       "          -0.7309, -0.1788,  0.1019,  0.3229,  1.0804, -1.7957, -0.7629,\n",
       "           0.7348,  0.1390],\n",
       "         [ 0.6896,  0.2331, -0.0244,  0.6727, -1.7574,  1.5368, -1.1478,\n",
       "           0.3341, -2.2361,  0.0247,  0.0626,  1.6984,  0.4240, -0.5726,\n",
       "          -0.3593, -1.2377],\n",
       "         [ 0.4424, -0.4761,  0.7644, -1.3690,  1.1007, -0.5165,  0.4082,\n",
       "           0.7773,  1.5334,  0.0933,  0.7493, -1.3580, -0.8301,  1.1550,\n",
       "           0.7490, -0.2990]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suppose we have the following parameters\n",
    "vocab_size = 100  # just an example\n",
    "d_model = 16     # embedding dimension\n",
    "max_seq_len = 5  # assume we have 5 tokens in a sequence\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "# Example: a batch of 2 sequences, each of length 5\n",
    "batch_token_indices = torch.tensor([\n",
    "    [1, 5, 2, 99, 3],\n",
    "    [10, 11, 12, 13, 14]\n",
    "])\n",
    "\n",
    "# Pass through the embedding layer\n",
    "embedded_output = embedding_layer(batch_token_indices)\n",
    "print(\"Embedded output shape:\", embedded_output.shape)\n",
    "embedded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 Integrating Positional Encoding**\n",
    "As discussed, Transformers add a **positional encoding** to the embedding:\n",
    "\\[\n",
    "z_i = e_i + PE_i,\\quad i = 1, 2, ..., n.\n",
    "\\]\n",
    "where \\( PE_i \\) is typically computed via sinusoidal functions at different frequencies.\n",
    "\n",
    "---\n",
    "## **3. Positional Encoding**\n",
    "Transformers do not have a built-in notion of sequence order (like RNNs do). To address this, the model uses **positional encodings** to inject information about the relative or absolute position of tokens in the sequence.\n",
    "\n",
    "A common choice from the paper is to use sine and cosine functions of different frequencies:\n",
    "\\[\n",
    "PE_{(pos,2i)} = \\sin\\Bigl(pos / 10^{4i/d_{model}}\\Bigr),\\quad\n",
    "PE_{(pos,2i+1)} = \\cos\\Bigl(pos / 10^{4i/d_{model}}\\Bigr),\n",
    "\\]\n",
    "where\n",
    "- \\( pos \\) is the position in the sequence (starting from 0, 1, 2, ...),\n",
    "- \\( i \\) indexes the dimension,\n",
    "- \\( d_{model} \\) is the dimension of the embeddings.\n",
    "\n",
    "This way, each position has a unique positional encoding vector of dimension \\( d_{model} \\). The final input representation for each token is:\n",
    "\\[\n",
    "z_i = e_i + PE_i,\n",
    "\\]\n",
    "where \\( PE_i \\) is the positional encoding for position \\( i \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Scaled Dot-Product Attention**\n",
    "The core idea behind the Transformer is the **self-attention** mechanism. Self-attention computes a set of **queries** (Q), **keys** (K), and **values** (V) from the input to decide how much each token should pay attention to the other tokens in the sequence.\n",
    "\n",
    "### **4.1 Formulas**\n",
    "Given \\( Q \\), \\( K \\), and \\( V \\) each of dimension \\( d_k \\), the attention output is:\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl(\\frac{Q K^T}{\\sqrt{d_k}}\\Bigr) V.\n",
    "\\]\n",
    "- \\( Q K^T \\) produces a score matrix (how relevant each query is to each key).\n",
    "- We scale by \\( \\sqrt{d_k} \\) to prevent large values when \\( d_k \\) is large.\n",
    "- We apply the softmax function to ensure the attention weights sum to 1.\n",
    "- Finally, we use these weights to produce a weighted sum of the values \\( V \\).\n",
    "\n",
    "### **4.2 Code Implementation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output Shape: torch.Size([2, 5, 16])\n",
      "Attention Weights Shape: torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q, K, V are of shape: (batch_size, seq_len, d_k)\n",
    "    Returns attention output and attention weights.\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)  # dimension of K\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    return output, attn_weights\n",
    "\n",
    "# Example usage:\n",
    "batch_size, seq_len, d_k = 2, 5, 16\n",
    "Q = torch.rand(batch_size, seq_len, d_k)\n",
    "K = torch.rand(batch_size, seq_len, d_k)\n",
    "V = torch.rand(batch_size, seq_len, d_k)\n",
    "\n",
    "attn_output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Attention Output Shape:\", attn_output.shape)\n",
    "print(\"Attention Weights Shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Multi-Head Attention**\n",
    "Instead of computing a single attention function, the Transformer uses **multiple attention heads** to capture different aspects of the relationships between tokens.\n",
    "\n",
    "- We have \\( h \\) heads, each with parameters \\( W_Q^{(i)}, W_K^{(i)}, W_V^{(i)} \\) (linear transformations for Q, K, V for head \\( i \\)).\n",
    "- Each head produces an output, which we then concatenate and project again.\n",
    "\n",
    "### **5.1 Formula**\n",
    "\\[\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\n",
    "\\]\n",
    "where\n",
    "- \\( \\text{head}_i = \\text{Attention}(Q W_Q^{(i)}, K W_K^{(i)}, V W_V^{(i)}) \\).\n",
    "- \\( W^O \\) is a final linear transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Position-wise Feed-Forward Network (FFN)**\n",
    "After the attention layers, each position in the sequence passes through a fully connected feed-forward network (applied independently to each position). The typical form is:\n",
    "\\[\n",
    "\\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2.\n",
    "\\]\n",
    "This helps in adding non-linearity and effectively learning transformations for each position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Putting It All Together**\n",
    "A single **Transformer Encoder** layer consists of:\n",
    "1. Multi-head self-attention sublayer (with residual connection & layer normalization).\n",
    "2. Position-wise feed-forward network sublayer (with residual connection & layer normalization).\n",
    "\n",
    "The **Transformer Decoder** has a similar structure, with an additional cross-attention sublayer that attends to the encoder output.\n",
    "\n",
    "Overall, the key takeaway from *Attention Is All You Need* is that attention mechanisms alone, without recurrences or convolutions, can achieve state-of-the-art performance in sequence modeling tasks, especially in NLP.\n",
    "\n",
    "### **7.1 Advantages**\n",
    "- **Parallelization**: Unlike RNNs, the Transformer can process all tokens in a sequence at once.\n",
    "- **Long-range dependencies**: Self-attention does not degrade over long distances as quickly as RNN-based models.\n",
    "- **Modular design**: Easy to scale by increasing model depth, width, or number of heads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762). *Advances in Neural Information Processing Systems*, 30.\n",
    "- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781). *ICLR*.\n",
    "- Pennington, J., Socher, R., & Manning, C. (2014). [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162). *EMNLP*.\n",
    "- Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606). *TACL*.\n",
    "\n",
    "---\n",
    "**End of Notebook**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
