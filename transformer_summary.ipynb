{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer (Attention Is All You Need) Overview**\n",
    "\n",
    "This notebook provides a concise summary of the seminal paper \"[Attention is All You Need (2017)](https://arxiv.org/abs/1706.03762)\" by Vaswani et al., which introduced the Transformer architecture. The goal is to explain the core concepts and mathematical formulations (in English) without any images.\n",
    "\n",
    "## **1. Introduction**\n",
    "Traditional sequence-to-sequence models relied on recurrent or convolutional structures, which made it difficult to process sequences in parallel and capture long-range dependencies efficiently. The Transformer architecture replaces these recurrent/convolutional components entirely with **attention mechanisms**, allowing for significantly improved parallelization and better performance on many NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Embedding**\n",
    "Before feeding the input tokens to the Transformer, they are transformed into dense vector representations known as **embeddings**. An embedding maps each token (e.g., a word, subword, or character) to a fixed-dimensional continuous vector space.\n",
    "\n",
    "- Let the input sequence be \\( x = (x_1, x_2, ..., x_n) \\).\n",
    "- We convert each token \\( x_i \\) into an embedding vector \\( e_i \\) of dimension \\( d_{model} \\).\n",
    "\n",
    "Mathematically, if \\( E \\) is the embedding matrix of size \\( |V| \\times d_{model} \\) (where \\( |V| \\) is vocabulary size), then\n",
    "\\[\n",
    "e_i = E[x_i],\\quad i = 1, 2, ..., n.\n",
    "\\]\n",
    "\n",
    "The embeddings are then supplemented with **positional encodings** to incorporate sequence order information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Positional Encoding**\n",
    "Transformers do not have a built-in notion of sequence order (like RNNs do). To address this, the model uses **positional encodings** to inject information about the relative or absolute position of tokens in the sequence.\n",
    "\n",
    "A common choice from the paper is to use sine and cosine functions of different frequencies:\n",
    "\\[\n",
    "PE_{(pos,2i)} = \\sin\\Bigl(pos / 10^{4i/d_{model}}\\Bigr),\\quad\n",
    "PE_{(pos,2i+1)} = \\cos\\Bigl(pos / 10^{4i/d_{model}}\\Bigr),\n",
    "\\]\n",
    "where\n",
    "- \\( pos \\) is the position in the sequence (starting from 0, 1, 2, ...),\n",
    "- \\( i \\) indexes the dimension,\n",
    "- \\( d_{model} \\) is the dimension of the embeddings.\n",
    "\n",
    "This way, each position has a unique positional encoding vector of dimension \\( d_{model} \\). The final input representation for each token is:\n",
    "\\[\n",
    "z_i = e_i + PE_i,\n",
    "\\]\n",
    "where \\( PE_i \\) is the positional encoding for position \\( i \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Scaled Dot-Product Attention**\n",
    "The core idea behind the Transformer is the **self-attention** mechanism. Self-attention computes a set of **queries** (Q), **keys** (K), and **values** (V) from the input to decide how much each token should pay attention to the other tokens in the sequence.\n",
    "\n",
    "### **4.1 Formulas**\n",
    "Given \\( Q \\), \\( K \\), and \\( V \\) each of dimension \\( d_k \\), the attention output is:\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl(\\frac{Q K^T}{\\sqrt{d_k}}\\Bigr) V.\n",
    "\\]\n",
    "- \\( Q K^T \\) produces a score matrix (how relevant each query is to each key).\n",
    "- We scale by \\( \\sqrt{d_k} \\) to prevent large values when \\( d_k \\) is large.\n",
    "- We apply the softmax function to ensure the attention weights sum to 1.\n",
    "- Finally, we use these weights to produce a weighted sum of the values \\( V \\).\n",
    "\n",
    "### **4.2 Code Implementation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q, K, V are of shape: (batch_size, seq_len, d_k)\n",
    "    Returns attention output and attention weights.\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)  # dimension of K\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    return output, attn_weights\n",
    "\n",
    "# Example usage:\n",
    "batch_size, seq_len, d_k = 2, 5, 16\n",
    "Q = torch.rand(batch_size, seq_len, d_k)\n",
    "K = torch.rand(batch_size, seq_len, d_k)\n",
    "V = torch.rand(batch_size, seq_len, d_k)\n",
    "\n",
    "attn_output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Attention Output Shape:\", attn_output.shape)\n",
    "print(\"Attention Weights Shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Multi-Head Attention**\n",
    "Instead of computing a single attention function, the Transformer uses **multiple attention heads** to capture different aspects of the relationships between tokens.\n",
    "\n",
    "- We have \\( h \\) heads, each with parameters \\( W_Q^{(i)}, W_K^{(i)}, W_V^{(i)} \\) (linear transformations for Q, K, V for head \\( i \\)).\n",
    "- Each head produces an output, which we then concatenate and project again.\n",
    "\n",
    "### **5.1 Formula**\n",
    "\\[\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\n",
    "\\]\n",
    "where\n",
    "- \\( \\text{head}_i = \\text{Attention}(Q W_Q^{(i)}, K W_K^{(i)}, V W_V^{(i)}) \\).\n",
    "- \\( W^O \\) is a final linear transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Position-wise Feed-Forward Network (FFN)**\n",
    "After the attention layers, each position in the sequence passes through a fully connected feed-forward network (applied independently to each position). The typical form is:\n",
    "\\[\n",
    "\\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2.\n",
    "\\]\n",
    "This helps in adding non-linearity and effectively learning transformations for each position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Putting It All Together**\n",
    "A single **Transformer Encoder** layer consists of:\n",
    "1. Multi-head self-attention sublayer (with residual connection & layer normalization).\n",
    "2. Position-wise feed-forward network sublayer (with residual connection & layer normalization).\n",
    "\n",
    "The **Transformer Decoder** has a similar structure, with an additional cross-attention sublayer that attends to the encoder output.\n",
    "\n",
    "Overall, the key takeaway from *Attention Is All You Need* is that attention mechanisms alone, without recurrences or convolutions, can achieve state-of-the-art performance in sequence modeling tasks, especially in NLP.\n",
    "\n",
    "### **7.1 Advantages**\n",
    "- **Parallelization**: Unlike RNNs, the Transformer can process all tokens in a sequence at once.\n",
    "- **Long-range dependencies**: Self-attention does not degrade over long distances as quickly as RNN-based models.\n",
    "- **Modular design**: Easy to scale by increasing model depth, width, or number of heads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762). *Advances in Neural Information Processing Systems*, 30.\n",
    "\n",
    "---\n",
    "**End of Notebook**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
